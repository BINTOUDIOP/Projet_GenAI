# orchestrator.py
from typing import TypedDict, Annotated, List
from operator import add
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver  # Checkpointer pour la m√©moire
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool

# Importation des outils et du RAG
from agent_tools import EXTERNAL_TOOLS
from finance_rag import run_rag_tool


# --- 1. D√©finition de l'√âtat du Graphe ---

class AgentState(TypedDict):
    """√âtat minimal requis pour la m√©moire et le routage LangGraph."""
    messages: Annotated[List[BaseMessage], add]
    thread_id: str


# --- 2. Initialisation du LLM et des Outils ---

# 1. Liste de tous les outils (RAG inclus)
ALL_TOOLS: List[BaseTool] = EXTERNAL_TOOLS + [run_rag_tool]

# 2. Initialisation du LLM avec tous les outils bind√©s (Tool Calling)
llm_with_all_tools = ChatOpenAI(model="gpt-4o-mini", temperature=0.0).bind_tools(ALL_TOOLS)

# üõë CORRECTION : Cr√©ation du map d'outils robuste pour l'ex√©cution üõë
TOOL_MAP = {}
for tool in ALL_TOOLS:
    # Pour les outils d√©finis avec @tool (comme le RAG), on utilise .func
    if hasattr(tool, 'func'):
        TOOL_MAP[tool.name] = tool.func
    # Pour les objets de classe (comme TavilySearch), on cherche .run ou .invoke
    elif hasattr(tool, 'run'):
        TOOL_MAP[tool.name] = tool.run
    elif hasattr(tool, 'invoke'):
        TOOL_MAP[tool.name] = tool.invoke
    else:
        # Mesure de s√©curit√© si un outil inconnu est pr√©sent
        raise AttributeError(
            f"Impossible de trouver la m√©thode d'ex√©cution (.func, .run, ou .invoke) pour l'outil {tool.name}. V√©rifiez la d√©finition.")


# --- 3. D√©finition des N≈ìuds (Actions) du Graphe ---

def run_llm_router(state: AgentState):
    """N≈ìud 1 : Le LLM d√©cide de la prochaine action (r√©ponse directe ou appel d'outil)."""
    # LangGraph passe l'historique complet via state["messages"]
    result = llm_with_all_tools.invoke(state["messages"])
    return {"messages": [result]}


def execute_tool(state: AgentState):
    """N≈ìud 2 : Ex√©cute l'outil s√©lectionn√© par le LLM et retourne les ToolMessage."""
    last_message = state["messages"][-1]  # AIMessage contenant les tool_calls
    tool_results = []

    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        args = tool_call["args"]

        # Utilisation du map pr√©d√©fini pour trouver la fonction
        tool_function = TOOL_MAP.get(tool_name)

        if tool_function:
            try:
                # Ex√©cution de la fonction/outil
                output = tool_function(**args)

                # Ajout du r√©sultat en tant que ToolMessage
                tool_results.append(
                    ToolMessage(
                        content=str(output),  # S'assurer que le contenu est une cha√Æne
                        tool_call_id=tool_call["id"],
                        name=tool_name
                    )
                )
            except Exception as e:
                # Gestion des erreurs pour √©viter le blocage du graphe
                tool_results.append(
                    ToolMessage(
                        content=f"Erreur d'ex√©cution de l'outil {tool_name}: {e}",
                        tool_call_id=tool_call["id"],
                        name=tool_name
                    )
                )
        else:
            tool_results.append(
                ToolMessage(
                    content=f"Outil {tool_name} inconnu. V√©rifiez agent_tools.py et finance_rag.py.",
                    tool_call_id=tool_call["id"],
                    name=tool_name
                )
            )

    return {"messages": tool_results}


# --- 4. D√©finition de la Logique de Routage ---

def should_continue(state: AgentState) -> str:
    """D√©termine la prochaine √©tape : 'tool' (boucle) ou 'end' (r√©ponse finale)."""
    last_message = state["messages"][-1]

    # Si le LLM a renvoy√© des tool_calls, nous devons ex√©cuter l'outil.
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "tool"

    # Sinon, le LLM a donn√© la r√©ponse finale, et nous terminons.
    return "end"


# --- 5. Construction du Graphe (LangGraph) ---

workflow = StateGraph(AgentState)
workflow.add_node("llm", run_llm_router)
workflow.add_node("tool", execute_tool)

# D√©finition de l'entr√©e et des transitions
workflow.set_entry_point("llm")
workflow.add_conditional_edges("llm", should_continue, {"tool": "tool", "end": END})
workflow.add_edge("tool", "llm")  # Apr√®s l'ex√©cution de l'outil, on revient toujours au LLM

# Compilation du graphe avec Checkpointer pour la gestion de la m√©moire
MEMORY = InMemorySaver()
app = workflow.compile(checkpointer=MEMORY)


# --- 6. Fonction d'Orchestration pour l'Interface ---

def run_orchestrator(question: str, thread_id: str) -> str:
    """Lance l'ex√©cution du LangGraph avec la question et la m√©moire (thread_id)."""

    # Configuration de la session (m√©moire) LangGraph
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}

    new_message = HumanMessage(content=question)

    # Invocation du graphe
    final_state = app.invoke({"messages": [new_message]}, config=config)

    # R√©cup√®re le contenu de la derni√®re r√©ponse (qui est toujours le message final du LLM)
    final_response = final_state["messages"][-1].content

    return final_response